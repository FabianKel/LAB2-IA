{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio 2**\n",
    "\n",
    "Derek Arreaga - 22537\n",
    "\n",
    "Paula Barillas - 22764\n",
    "\n",
    "Mónica Salavatierra - 22249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LINK DE REPOSITORIO\n",
    "https://github.com/FabianKel/LAB2-IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TASK #1 - Preguntas Teóricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "- El modelo se considera naive debido a que asume que las características de medición son independientes entre sí y contribuyen por igual en el momento de dar resultado. Esta suposición de independencia es rara vez es cierta. Sin embargo, permite que el modelo sea muy eficiente en términos de tiempo de computación y memoria.\n",
    "\n",
    "### Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda ¿cómo funciona el truco del Kernel para este modelo? (Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)\n",
    "\n",
    "- La formulación matemática que se busca optimizar en Support Vector Machine (SVM) es maximizar el margen entre las clases o bien como fue mencionado en clase crear una frontera para separar las diferentes clases con las que estamos logrando. Esto se logra resolviendo el problema de optimización cuadrática donde se aseguran que los datos estén correctamente clasificados y fuera del margen. La función es:\n",
    "\n",
    "$$\\min_{(\\mathbf{w}, b)} \\frac{1}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "sujeto a:\n",
    "\n",
    "$$y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i$$\n",
    "\n",
    "donde $\\mathbf{w}$ es el vector de pesos, $b$ es el sesgo, $\\mathbf{x}_i$ son los vectores de características y $y_i$ son las etiquetas de clase.\n",
    "\n",
    "- El truco del Kernel permite a SVM manejar datos no linealmente separables al transformar los datos originales a un espacio de mayor dimensión donde es más probable que sean linealmente separables. Esto se hace utilizando una función de kernel $(K(\\mathbf{x}_i, \\mathbf{x}_j))$ que va calcula el producto punto en el espacio transformado sin necesidad de calcular explícitamente la transformación.\n",
    "### Investigue sobre Random Forest y responda\n",
    "1. ¿Qué tipo de ensemble learning es este modelo?\n",
    "    - El tipo de ensemble learning es Bagging (Bootstrap Aggregating).\n",
    "\n",
    "2. ¿Cuál es la idea general detrás de Random Forest?\n",
    "    - La idea general detrás de Random Forest es construir múltiples árboles de decisión durante el entrenamiento y clasificación o la media de las predicciones (regresión) de los árboles individuales. Cada árbol es entrenado con una muestra aleatoria del dataset original con reemplazo (bootstrap sample), y en cada nodo de los árboles, solo un subconjunto aleatorio de características es considerado para la división.\n",
    "\n",
    "3. ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "    - Se busca baja correlación entre los árboles de Random Forest porque si los árboles son altamente correlacionados, sus errores también serán correlacionados, lo que reduce la efectividad del ensemble. La baja correlación asegura que los errores de los árboles individuales se cancelen entre sí, mejorando la precisión y robustez del modelo final.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TASK #2 - Naive Bayes: Clasificador de Mensajes Ham/Spam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de archivo a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "Distribución de clases:\n",
      "label\n",
      "ham     4818\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    messages = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            label, message = line.strip().split('\\t', 1)\n",
    "            messages.append(message)\n",
    "            labels.append(label)\n",
    "    \n",
    "    df = pd.DataFrame({\"label\": labels, \"message\": messages})\n",
    "    \n",
    "    return df  \n",
    "\n",
    "# Cargar el dataset en un df\n",
    "df = load_dataset('entrenamiento.txt')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Verificar la distribución de clases\n",
    "print(\"\\nDistribución de clases:\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2.1 Limpieza y lectura de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5565 entries, 0 to 5564\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5565 non-null   object\n",
      " 1   message  5565 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5565</td>\n",
       "      <td>5565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4818</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5565                    5565\n",
       "unique     2                    5153\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4818                      30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  go until jurong point crazy available only in ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry in a wkly comp to win fa cup final ...\n",
      "3   ham        u dun say so early hor u c already then say\n",
      "4   ham  nah i dont think he goes to usf he lives aroun...\n",
      "5  spam  freemsg hey there darling its been weeks now a...\n",
      "6   ham  even my brother is not like to speak with me t...\n",
      "7   ham  as per your request melle melle oru minnaminun...\n",
      "8  spam  winner as a valued network customer you have b...\n",
      "9  spam  had your mobile months or more u r entitled to...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "df[\"message\"] = df[\"message\"].apply(preprocess_text)\n",
    "\n",
    "# Convertir listas de palabras en cadenas separadas por espacios\n",
    "df[\"message\"] = df[\"message\"].apply(lambda words: \" \".join(words))\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset dividido en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 4452\n",
      "Tamaño del conjunto de prueba: 1113\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Training y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"message\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(X_test)}\")\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2.2 - Construcción del Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 7533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def TrainNaive_bayes(messages, labels, alpha=0.5):\n",
    "\n",
    "    #contadores\n",
    "    word_counts = {\"ham\": {}, \"spam\": {}}  # Diccionario para contar palabras por clase\n",
    "    class_counts = {\"ham\": 0, \"spam\": 0}  # Contador de mensajes por clase\n",
    "    vocabulary = set()  # Conjunto de palabras únicas\n",
    "\n",
    "    # Recorrer los mensajes y contar palabras por clase\n",
    "    for i in range(len(messages)):  \n",
    "        message = messages[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Contar cuántos mensajes hay en cada clase\n",
    "        class_counts[label] += 1\n",
    "\n",
    "        words = preprocess_text(message)\n",
    "\n",
    "        # Contar palabras en cada categoría\n",
    "        for word in words:\n",
    "            if word not in word_counts[label]:\n",
    "                word_counts[label][word] = 0\n",
    "            word_counts[label][word] += 1\n",
    "            vocabulary.add(word)  \n",
    "\n",
    "    # Calcular probabilidades de cada clase\n",
    "    total_messages = len(messages)\n",
    "    class_probs = {label: class_counts[label] / total_messages for label in class_counts}\n",
    "\n",
    "    # Calcular probabilidades de cada palabra \n",
    "    vocab_size = len(vocabulary) \n",
    "    word_probs = {\"ham\": {}, \"spam\": {}}\n",
    "\n",
    "    for label in class_counts.keys():\n",
    "        total_words = sum(word_counts[label].values())  \n",
    "        \n",
    "        for word in vocabulary:\n",
    "            # Aplicar Laplace Smoothing\n",
    "            word_count = word_counts[label].get(word, 0)  \n",
    "            word_probs[label][word] = (word_count + alpha) / (total_words + alpha * vocab_size)\n",
    "\n",
    "    return word_probs, class_probs, list(vocabulary)\n",
    "\n",
    "# Convertir X_train y y_train en listas \n",
    "X_train = list(X_train) if isinstance(X_train, pd.Series) else X_train\n",
    "y_train = list(y_train) if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "word_probs, class_probs, vocabulary = TrainNaive_bayes(X_train, y_train)\n",
    "\n",
    "# estadísticas del modelo entrenado\n",
    "print(\"Tamaño del vocabulario:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Justificación de la métrica de desempeño a utilizar**\n",
    "Tomando en cuenta que el dataset se encuentra desbalanceado (existe una mayor cantidad de mensajes ham que spam), se utilizará la métrica de **recall** para medir el desempeño del modelo:\n",
    "- Al obtener métricas sólidas y favorables y entender que tan bien se clasifican los mensajes, podremos aplicar pesos para mejorar el rendimiento del modelo en la detección de la clase minoritaria (spam).\n",
    "- El recall es clave en este caso, ya que nos interesa minimizar los falsos negativos en la detección de mensajes spam, evitando que estos sean clasificados incorrectamente como ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenamiento manual  ===\n",
      "\n",
      "=== Evaluación en conjunto de entrenamiento ===\n",
      "\n",
      "Métricas de evaluación:\n",
      "- Recall para spam: 0.980\n",
      "- Recall para ham: 0.969\n",
      "- Recall promedio: 0.974\n",
      "\n",
      "=== Evaluación en conjunto de prueba ===\n",
      "\n",
      "Métricas de evaluación:\n",
      "- Recall para spam: 0.933\n",
      "- Recall para ham: 0.967\n",
      "- Recall promedio: 0.950\n"
     ]
    }
   ],
   "source": [
    "manual_class_prior = {\n",
    "    \"ham\": 0.3,  \n",
    "    \"spam\": 0.7\n",
    "}\n",
    "\n",
    "def evaluate_naive_bayes(X_test, y_test, word_probs, class_probs, vocabulary, class_prior):\n",
    "    # contadores para la matriz de confusión\n",
    "    true_pos_spam = 0\n",
    "    false_neg_spam = 0\n",
    "    true_pos_ham = 0\n",
    "    false_neg_ham = 0\n",
    "    \n",
    "    # Evaluar cada mensaje\n",
    "    for message, true_label in zip(X_test, y_test):\n",
    "        words = preprocess_text(message)\n",
    "        \n",
    "        scores = {\n",
    "            \"ham\": math.log(class_prior[\"ham\"]),  \n",
    "            \"spam\": math.log(class_prior[\"spam\"])\n",
    "        }\n",
    "        \n",
    "        # Calcular probabilidad para cada clase usando Laplace Smoothing\n",
    "        for label in [\"ham\", \"spam\"]:\n",
    "            for word in words:\n",
    "                if word in vocabulary:\n",
    "                    scores[label] += math.log(word_probs[label][word])\n",
    "        \n",
    "        # Determinar predicción \n",
    "        pred_label = max(scores, key=scores.get)\n",
    "        \n",
    "        # Actualizar contadores\n",
    "        if true_label == \"spam\":\n",
    "            if pred_label == \"spam\":\n",
    "                true_pos_spam += 1\n",
    "            else:\n",
    "                false_neg_spam += 1\n",
    "        else:  # true_label == \"ham\"\n",
    "            if pred_label == \"ham\":\n",
    "                true_pos_ham += 1\n",
    "            else:\n",
    "                false_neg_ham += 1\n",
    "    \n",
    "    # Calcular métricas\n",
    "    recall_spam = true_pos_spam / (true_pos_spam + false_neg_spam) if (true_pos_spam + false_neg_spam) > 0 else 0\n",
    "    recall_ham = true_pos_ham / (true_pos_ham + false_neg_ham) if (true_pos_ham + false_neg_ham) > 0 else 0\n",
    "    avg_recall = (recall_spam + recall_ham) / 2\n",
    "    \n",
    "    metrics = {\n",
    "        \"recall_spam\": recall_spam,\n",
    "        \"recall_ham\": recall_ham,\n",
    "        \"avg_recall\": avg_recall,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\\n=== Entrenamiento manual  ===\")\n",
    "print(\"\\n=== Evaluación en conjunto de entrenamiento ===\")\n",
    "train_metrics = evaluate_naive_bayes(X_train, y_train, word_probs, class_probs, vocabulary, manual_class_prior)\n",
    "print(f\"\\nMétricas de evaluación:\")\n",
    "print(f\"- Recall para spam: {train_metrics['recall_spam']:.3f}\")\n",
    "print(f\"- Recall para ham: {train_metrics['recall_ham']:.3f}\")\n",
    "print(f\"- Recall promedio: {train_metrics['avg_recall']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Evaluación en conjunto de prueba ===\")\n",
    "test_metrics = evaluate_naive_bayes(X_test, y_test, word_probs, class_probs, vocabulary, manual_class_prior)\n",
    "print(f\"\\nMétricas de evaluación:\")\n",
    "print(f\"- Recall para spam: {test_metrics['recall_spam']:.3f}\")\n",
    "print(f\"- Recall para ham: {test_metrics['recall_ham']:.3f}\")\n",
    "print(f\"- Recall promedio: {test_metrics['avg_recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2.3 - Clasificación de mensajes futuros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Clasificador de Mensajes ===\n",
      "\n",
      "Resultados de la clasificación:\n",
      "Mensaje ingresado: \"Are you ready for tonight's party?\"\n",
      "Probabilidad de spam: 0.30%\n",
      "Probabilidad de ham: 99.70%\n",
      "Clasificación final: HAM\n",
      "\n",
      "=== Clasificador de Mensajes ===\n",
      "\n",
      "Resultados de la clasificación:\n",
      "Mensaje ingresado: \"Click here to claim your prize!\"\n",
      "Probabilidad de spam: 100.00%\n",
      "Probabilidad de ham: 0.00%\n",
      "Clasificación final: SPAM\n",
      "\n",
      "=== Clasificador de Mensajes ===\n",
      "\n",
      "Resultados de la clasificación:\n",
      "Mensaje ingresado: \"Congratulations! You won a new car!\"\n",
      "Probabilidad de spam: 99.56%\n",
      "Probabilidad de ham: 0.44%\n",
      "Clasificación final: SPAM\n",
      "\n",
      "=== Clasificador de Mensajes ===\n"
     ]
    }
   ],
   "source": [
    "def classify_message(message, word_probs, class_probs, vocabulary):\n",
    "\n",
    "    words = preprocess_text(message)\n",
    "    \n",
    "    # Calcular score para cada clase\n",
    "    scores = {\"ham\": math.log(class_probs[\"ham\"]), \n",
    "            \"spam\": math.log(class_probs[\"spam\"])}\n",
    "    \n",
    "    # Para cada palabra en el mensaje\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            for label in [\"ham\", \"spam\"]:\n",
    "                # Multiplicar las probabilidades (se puede realizar una suma de logaritmos)\n",
    "                scores[label] += math.log(word_probs[label][word])\n",
    "    \n",
    "    # Convertir scores logarítmicos a probabilidades\n",
    "    total = sum(math.exp(score) for score in scores.values())\n",
    "    probs = {label: math.exp(score)/total for label, score in scores.items()}\n",
    "    \n",
    "    # Determinar clasificación\n",
    "    classification = max(probs.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return classification, probs[\"spam\"], probs[\"ham\"]\n",
    "\n",
    "\n",
    "def classify_interface():\n",
    "    while True:\n",
    "        print(\"\\n=== Clasificador de Mensajes ===\")\n",
    "        message = input(\"\\nIngrese el mensaje a clasificar (o 'q' para salir): \")\n",
    "        \n",
    "        if message.lower() == 'q':\n",
    "            break\n",
    "            \n",
    "        # Clasificar el mensaje\n",
    "        classification, spam_prob, ham_prob = classify_message(\n",
    "            message, word_probs, class_probs, vocabulary\n",
    "        )\n",
    "        \n",
    "        print(\"\\nResultados de la clasificación:\")\n",
    "        print(f\"Mensaje ingresado: \\\"{message}\\\"\")\n",
    "        print(f\"Probabilidad de spam: {spam_prob:.2%}\")\n",
    "        print(f\"Probabilidad de ham: {ham_prob:.2%}\")\n",
    "        print(f\"Clasificación final: {classification.upper()}\")\n",
    "\n",
    "classify_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2.4 - Entrenamiento con librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenamiento con librerías ===\n",
      "\n",
      "=== Evaluación en conjunto de entrenamiento ===\n",
      "- Recall spam: 0.987\n",
      "- Recall ham: 0.969\n",
      "- Recall promedio: 0.978\n",
      "\n",
      "=== Evaluación en conjunto de prueba ===\n",
      "- Recall spam: 0.933\n",
      "- Recall ham: 0.966\n",
      "- Recall promedio: 0.949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Vectorizar el texto (convertir a matriz de características)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "sklearn_model = MultinomialNB(alpha=0.5, class_prior=[0.3, 0.7])\n",
    "sklearn_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_train_pred = sklearn_model.predict(X_train_vectorized)\n",
    "y_test_pred = sklearn_model.predict(X_test_vectorized)\n",
    "\n",
    "# Calcular recall para cada conjunto\n",
    "sklearn_train_recall_spam = recall_score(y_train, y_train_pred, pos_label='spam')\n",
    "sklearn_train_recall_ham = recall_score(y_train, y_train_pred, pos_label='ham')\n",
    "sklearn_train_recall_avg = recall_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "sklearn_test_recall_spam = recall_score(y_test, y_test_pred, pos_label='spam')\n",
    "sklearn_test_recall_ham = recall_score(y_test, y_test_pred, pos_label='ham')\n",
    "sklearn_test_recall_avg = recall_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "\n",
    "print(\"\\n=== Entrenamiento con librerías ===\")\n",
    "print(\"\\n=== Evaluación en conjunto de entrenamiento ===\")\n",
    "print(f\"- Recall spam: {sklearn_train_recall_spam:.3f}\")\n",
    "print(f\"- Recall ham: {sklearn_train_recall_ham:.3f}\")\n",
    "print(f\"- Recall promedio: {sklearn_train_recall_avg:.3f}\")\n",
    "print(\"\\n=== Evaluación en conjunto de prueba ===\")\n",
    "print(f\"- Recall spam: {sklearn_test_recall_spam:.3f}\")\n",
    "print(f\"- Recall ham: {sklearn_test_recall_ham:.3f}\")\n",
    "print(f\"- Recall promedio: {sklearn_test_recall_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **¿Cuál implementación lo hizo mejor? ¿Su implementación o la de la librería?**\n",
    "- Ambas implementaciones lograron métricas de recall muy similares en el conjunto de prueba, con diferencias mínimas.\n",
    "\n",
    "    - La implementación manual obtuvo un recall promedio de 0.950 en el conjunto de prueba.\n",
    "    - La implementación con librerías obtuvo un recall promedio de 0.949 en el conjunto de prueba.\n",
    "    - En general, la implementación manual tuvo un recall ligeramente superior en la prueba, especialmente en spam (0.980 vs. 0.987 en entrenamiento y 0.933 en prueba para ambas metodologías).\n",
    "\n",
    "    Dado que la diferencia entre ambas versiones es muy pequeña, ambas implementaciones pueden considerarse efectivas para la clasificación de mensajes spam/ham.\n",
    "### **¿Por qué cree que se debe esta diferencia?**\n",
    "- Las diferencias generadas en los valores pueden deberse a varios factores como el cálculo de las probabilidades y la precisión del cálculo de los logaritmos como tal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TASK #3 - Clasificación de Partidas de League of Legends**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Referencias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es Support Vector Machine? (2024, octubre 4). Ibm.com. https://www.ibm.com/mx-es/topics/support-vector-machine\n",
    "\n",
    "susmit_sekhar_bhakta Follow Improve. (2024, febrero 22). Random Forest algorithm in machine learning. GeeksforGeeks. https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/\n",
    "\n",
    "What is random forest? (2024, diciembre 19). Ibm.com. https://www.ibm.com/think/topics/random-forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
